{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "All classes and functions, which can be imported into the training or evaluation scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for this if running on a CPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import VarianceScaling\n",
    "from keras.layers import (Conv2D, Dense, Flatten, Input, Lambda)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, replay_buffer, policy_dqn, target_dqn, n_actions, train_batch_size,\n",
    "                 input_shape, eps_start=1.0, eps_min=0.1, eps_decay=0.996,\n",
    "                 discount_factor=0.99, debug=False):\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.policy_dqn = policy_dqn\n",
    "        self.target_dqn = target_dqn\n",
    "        self.eps_cur = eps_start\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_decay = eps_decay\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = train_batch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.debug = debug\n",
    "        \n",
    "        # copy weights from policy network into target\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # draw random number between 0 and 1 and if it's lower than the \n",
    "        # current epsilon (exploration rate), then choose a random action,\n",
    "        # otherwise use NN to predict the action using the optimal policy\n",
    "        random_n = np.random.random()\n",
    "        if random_n < self.eps_cur:\n",
    "            # explore environment, basically choose random action\n",
    "            action = np.random.choice(self.n_actions)\n",
    "            process_type = 'RandomChoice'\n",
    "        else:\n",
    "            # here we are not exploring, but rather exploiting the knowledge\n",
    "            state = state[np.newaxis, :]  # extend dimension by +1, as we need a row vector for NN\n",
    "            actions = self.policy_dqn.predict(state)\n",
    "            action = np.argmax(actions[0])\n",
    "            process_type = 'NN'\n",
    "        if self.debug:\n",
    "            # print(f'Action chosen: {action} by {process_type}')\n",
    "            pass\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def add_experience(self, action, state, reward, new_state, terminal):\n",
    "        self.replay_buffer.add_experience(action, state, reward, new_state, terminal)\n",
    "        \n",
    "    def learn(self, frame_number):\n",
    "        # only learn if we've reached a minimum number of processed frames\n",
    "        if frame_number < self.batch_size:           \n",
    "            return\n",
    "        # get sample minibatch from replay buffer:\n",
    "        actions, states, rewards, new_states, \\\n",
    "            terminal_flags = self.replay_buffer.sample_minibatch(self.batch_size)\n",
    "        \n",
    "        # Learning: fit Policy DQN\n",
    "        # targetQ according to Bellman equation: \n",
    "        # Q = r + gamma*max Q'\n",
    "        if self.debug:\n",
    "            # print(f'Train at frame {frame_number}')\n",
    "            pass\n",
    "        \n",
    "        # Policy Network: estimate which action is the best for new states (s')\n",
    "        future_policy_q_vals = self.policy_dqn.predict(new_states)\n",
    "        arg_q_max = future_policy_q_vals.argmax(axis=1)\n",
    "        \n",
    "        # Target Network: estimate Q-values for new states (s')\n",
    "        future_target_q_vals = self.target_dqn.predict(new_states)\n",
    "        \n",
    "        # Policy Network: feed forward to retrieve Q-Values for current states (s)\n",
    "        cur_policy_q_vals = self.policy_dqn.predict(states)\n",
    "        \n",
    "        # create a copy of Q-values estimated for current state (s)\n",
    "        q_targets = cur_policy_q_vals[:]\n",
    "        \n",
    "        # extract future Q-values for the max actions in current state\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        double_q = future_target_q_vals[batch_index, arg_q_max]\n",
    "        \n",
    "        # update Q-values for actions selected for current state\n",
    "        # using Bellman equation\n",
    "        q_targets[batch_index, actions] = reward + \\\n",
    "            self.discount_factor * double_q * terminal_flags\n",
    "        \n",
    "        # fit Policy Network:\n",
    "        # X: current states\n",
    "        # y: new Q-Targets\n",
    "        _ = self.policy_dqn.fit(states, q_targets, verbose=0)\n",
    "        \n",
    "    def dec_epsilon(self):\n",
    "        # decrement epsilon by eps_decay value\n",
    "        self.eps_cur = self.eps_cur * self.eps_decay\n",
    "        self.eps_cur = np.max([self.eps_min, self.eps_cur])\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        if self.debug:\n",
    "            # print('Update Target Net weights')\n",
    "            pass\n",
    "        self.target_dqn.set_weights(self.policy_dqn.get_weights())\n",
    "\n",
    "    def save_model(self, to_file):\n",
    "        self.policy_dqn.save(to_file)\n",
    "\n",
    "    def load_model(self, from_file):\n",
    "        self.policy_dqn = load_model(from_file)\n",
    "        if self.eps_cur <= self.eps_min:\n",
    "            self.update_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameWrapper(object):\n",
    "    \n",
    "    def __init__(self, env_name, debug=False):\n",
    "        self.env = gym.make(env_name).unwrapped\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.n_obsevations = self.env.observation_space.shape\n",
    "        self.state = None\n",
    "        self.debug = debug\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.env.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Return reward and done boolean\n",
    "        \"\"\"\n",
    "        self.state, reward, done, _ = self.env.step(action)\n",
    "        return reward, done\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, buffer_size, n_actions, input_shape, debug=False):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.experience_counter = 0\n",
    "        self.actions_memory = np.zeros((buffer_size), dtype=np.int32)\n",
    "        self.states_memory = np.zeros((buffer_size, *input_shape))\n",
    "        self.new_states_memory = np.zeros((buffer_size, *input_shape))\n",
    "        self.rewards_memory = np.zeros(buffer_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(buffer_size, dtype=np.uint8)\n",
    "        self.debug = debug\n",
    "        \n",
    "    def add_experience(self, action, state, reward, new_state, terminal):\n",
    "        idx = np.mod(self.experience_counter, self.buffer_size)  # module will give us the current index to use\n",
    "        self.actions_memory[idx] = action\n",
    "        self.states_memory[idx] = state\n",
    "        self.rewards_memory[idx] = reward\n",
    "        self.new_states_memory[idx] = new_state\n",
    "        self.terminal_memory[idx] = 1 - int(terminal)\n",
    "        self.experience_counter += 1\n",
    "        # print debug info\n",
    "        if self.debug:\n",
    "            print(f'{self.experience_counter} experience(s) added to replay buffer')\n",
    "\n",
    "    def sample_minibatch(self, batch_size):\n",
    "        # check how many experiences are filled\n",
    "        n_take = np.min([self.experience_counter, self.buffer_size])\n",
    "        # sample random N-elements\n",
    "        idx = np.random.choice(n_take, batch_size)\n",
    "        # print debug info\n",
    "        if self.debug:\n",
    "            print(f'Minibatch of {n_take} experiences sampled')\n",
    "        # return tuple of sampled experiences\n",
    "        return (self.actions_memory[idx], self.states_memory[idx], self.rewards_memory[idx],\n",
    "                self.new_states_memory[idx], self.terminal_memory[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(input_shape, n_actions, lr=0.001, debug=False):\n",
    "    \n",
    "    # Define network layers\n",
    "    model_input = Input(shape=(input_shape))\n",
    "    x = Dense(256, activation='relu')(model_input)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(n_actions, activation='linear')(x)  # linear is default, but let's be explicit        \n",
    "    \n",
    "    # Build model\n",
    "    model = Model(model_input, x)\n",
    "    model.compile(Adam(lr), loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
