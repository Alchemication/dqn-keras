{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam Napora\n",
    "- 18197892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This is an evaluation script for the implementation of Deep Q-Learning algorithm heavily inspired by this post: https://medium.com/analytics-vidhya/building-a-powerful-dqn-in-tensorflow-2-0-explanation-tutorial-d48ea8f3177a.\n",
    "\n",
    "Most important parts of the evaluation script:\n",
    "- we are using an epsilon value set to 0, so the Agent chooses to exploit the learned actions, instead of randomly exploring the environment's state\n",
    "- we are pointing the script to the latest training snapshot (using **RESTORE_PATH** variable)\n",
    "- I did not commit the latest snapshot to the repository as the models are just too big (13MB and 36MB), also the replay buffer is very large (7GB)\n",
    "- my final model, after over 2 days of training was able to consistenly achieve a score of 50, which is around a human (or better) level of performance\n",
    "- I have noticed a glitch where the UI gets stuck when we stop the process and the window either needs to be killed from Terminal or Jupyter needs to be restarted all together (I've used Jupyter Lab on my own local machine for this project)\n",
    "\n",
    "**Note:** Please see the full project and Training description in the main Training Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Generic Libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RL Application objects (described in detail in the other - Training Notebook)\n",
    "from rl_imports import build_q_network\n",
    "from rl_imports import GameWrapper\n",
    "from rl_imports import ReplayBuffer\n",
    "from rl_imports import Agent\n",
    "from rl_imports import (BATCH_SIZE, CLIP_REWARD, DISCOUNT_FACTOR, ENV_NAME,\n",
    "                    EVAL_LENGTH, FRAMES_BETWEEN_EVAL, INPUT_SHAPE,\n",
    "                    LEARNING_RATE, LOAD_FROM, MAX_EPISODE_LENGTH,\n",
    "                    MAX_NOOP_STEPS, MEM_SIZE, MIN_REPLAY_BUFFER_SIZE,\n",
    "                    SAVE_PATH, TOTAL_FRAMES, UPDATE_FREQ, WRITE_TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "Loading model...\n",
      "Loaded\n",
      "Game over, reward: 64.0, frame: 1319/10000\n"
     ]
    }
   ],
   "source": [
    "# My installations require I run this to avoid errors with cuDNN.\n",
    "# You can remove it if your system doesn't require it.\n",
    "# (it shouldn't mess anything up if you keep it in)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Change this to the path of the model you would like to visualize\n",
    "RESTORE_PATH = './breakout-saves/save-04319620'\n",
    "\n",
    "# Create environment\n",
    "game_wrapper = GameWrapper(ENV_NAME, MAX_NOOP_STEPS)\n",
    "print(\"The environment has the following {} actions: {}\".format(game_wrapper.env.action_space.n, game_wrapper.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "# Create agent\n",
    "MAIN_DQN = build_q_network(game_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)\n",
    "TARGET_DQN = build_q_network(game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=MEM_SIZE, input_shape=INPUT_SHAPE)\n",
    "agent = Agent(MAIN_DQN, TARGET_DQN, replay_buffer, game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE, \n",
    "              batch_size=BATCH_SIZE)\n",
    "\n",
    "print('Loading model...')\n",
    "agent.load(RESTORE_PATH)\n",
    "print('Loaded')\n",
    "\n",
    "terminal = True\n",
    "eval_rewards = []\n",
    "\n",
    "for frame in range(EVAL_LENGTH):\n",
    "    if terminal:\n",
    "        game_wrapper.reset(evaluation=True)\n",
    "        life_lost = True\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "\n",
    "    # Breakout require a \"fire\" action (action #1) to start the\n",
    "    # game each time a life is lost.\n",
    "    # Otherwise, the agent would sit around doing nothing.\n",
    "    action = 1 if life_lost else agent.get_action(0, game_wrapper.state, evaluation=True)\n",
    "\n",
    "    # Step action\n",
    "    _, reward, terminal, life_lost = game_wrapper.step(action, render_mode='human')\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    episode_reward_sum += reward\n",
    "\n",
    "    # On game-over\n",
    "    if terminal:\n",
    "        print(f'Game over, reward: {episode_reward_sum}, frame: {frame}/{EVAL_LENGTH}')\n",
    "        eval_rewards.append(episode_reward_sum)\n",
    "\n",
    "print('Average reward:', np.mean(eval_rewards) if len(eval_rewards) > 0 else episode_reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
